config_path: ./configs/main.yaml
environment:
  distance_to_goal_penalty: 0.5
  enable_keyboard: false
  environment_type: SLOPE
  goal_step: 100
  goal_type: distance
  peak_position:
  - 1.15
  - 1.15
  - 0.3
  render_mode: GUI
  target_velocity_change: 0.5
  time_penalty: 0.1
  time_step_size: 1/20
  video_mode: false
  x_distance_to_goal: 100
epsilon_greedy:
  eps_decay: 0.001
  eps_end: 0.05
  eps_init: 0.95
  epsilon_decay_type: exponential
  stretched_A: 0.5
  stretched_B: 0.1
  stretched_C: 0.1
gpu_id: '0'
hyperparameter_tuning:
  tuning_variable: learning_rate_actor
  type: model
  value_list:
  - 0.0003
  - 0.0001
  - 1.0e-05
load_model_weights_path: ./results/train/DQN_trainING/DQN_Train_UnevenTerrainTWO\best_model.pt
mode: train
model:
  batch_size: 64
  dropout_rate:
  - 0
  - 0
  - 0
  gamma: 0.99
  hidden_layer_size:
  - 128
  - 128
  - 128
  learning_rate_actor: 0.0003
  learning_rate_critic: 0.0003
  load_model_weights_path: ./results/train/DQN_robot_weight/best_model.pt
  name: DQN
  tau: 0.005
  weight_decay:
  - 0
  - 0
  - 0
plotting:
  plot_trajectories_episode_interval: 100
  record_trajectory_time_step_interval: 5
reward_function:
  end_goal_bonus: 10.0
  progress_reward: 0.5
  stability_penalty: 0.02
run:
  name: DQN_Train_UnevenTerrainTWO
testing:
  base_results_dir: ./results/test/DQN_test/
  device: auto
  max_steps_per_episode: 50000
  num_test_episodes: 1000
  num_workers: 40
  record_video: true
training:
  base_results_dir: ./results/train/DQN_trainING/
  device: cpu
  max_steps_per_episode: 1000
  n_step: false
  num_train_episodes: 3000
  num_workers: 40
  save_model_weights: true
