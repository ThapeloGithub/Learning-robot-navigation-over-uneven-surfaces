# ======== CONFIGURATION FILE FOR TRAINING ========
# RENAME THE RUN NAME FOR A NEW RUN

# ======= RUN ======= #
run: 
  name: DQN_Train_UnevenTerrainTWO

# ======= MODEL ======= #
model:
  name: DQN
  learning_rate_actor: 3.0e-4  # Adjusted for SAC
  learning_rate_critic: 3.0e-4  # Adjusted for SAC
  batch_size: 64
  gamma: 0.99
  hidden_layer_size: [128, 128, 128]
  weight_decay: [0, 0, 0]
  dropout_rate: [0, 0, 0]
  tau: 0.005
  load_model_weights_path: "./results/train/DQN_robot_weight/best_model.pt"  # Set path for loading weights, if available

# ======= EPSILON GREEDY ======= #
epsilon_greedy:
  eps_init: 0.95
  eps_end: 0.05
  eps_decay: 0.001  # Faster decay for quicker transition to exploitation
  epsilon_decay_type: 'exponential'  # Options: linear, exponential, stretched
  stretched_A: 0.5
  stretched_B: 0.1
  stretched_C: 0.1

# ======= TRAINING ======= #
training:
  device: 'cpu'  # Options: auto, cpu, cuda:0
  num_workers: 40  # Number of CPU cores for parallel computation
  num_train_episodes: 3000
  max_steps_per_episode: 1000  # Reduced for quicker feedback during training
  n_step: False
  base_results_dir: './results/train/DQN_trainING/'
  save_model_weights: True

# ======= TESTING ======= #
testing:
  device: 'auto'  # Options: auto, cpu, cuda:0
  num_workers: 40
  num_test_episodes: 1000
  max_steps_per_episode: 50000
  base_results_dir: './results/test/DQN_test/'
  record_video: True

# ======= ENVIRONMENT ======= #
environment:
  render_mode: 'GUI'  # Options: DIRECT, GUI
  video_mode: False
  enable_keyboard: False
  environment_type: 'SLOPE'
  goal_type: 'distance'  # Ensure the environment supports this
  x_distance_to_goal: 100 
  goal_step: 100
  time_step_size: 1/20
  distance_to_goal_penalty: 0.5  # Reduced penalty to allow progress
  time_penalty: 0.1  # Reduced penalty to encourage exploration
  target_velocity_change: 0.5
  peak_position: [1.15, 1.15, 0.3]  # Ensure this matches your environment

# ======= PLOTTING ======= #
plotting:
  plot_trajectories_episode_interval: 100
  record_trajectory_time_step_interval: 5

# ======= HYPERPARAMETER TUNING ======= #
hyperparameter_tuning:
  type: model  # Options: epsilon_greedy, model, environment
  tuning_variable: learning_rate_actor  # Variable to tune
  value_list: [3.0e-4, 1.0e-4, 1.0e-5]  # Range of values for tuning

# ======= REWARD FUNCTION ======= #
reward_function:
  progress_reward: 0.5  # Increased incentive for moving toward the goal
  stability_penalty: 0.02  # Slight penalty to allow instability
  end_goal_bonus: 10.0  # Increased reward for reaching the goal
